---
title: 'Nightshade'
date: '2024-08-24'
---

I started my research into digital watermarking with LSB steganography. Where the least significant bits of each RGB pixel store binary data, when stitched together, creates a message of some kind. Sony’s A7 line plays around with a similar idea in their 2022~ releases, coining it as a cryptographic anti-forgery mechanism. Definitely more robust, but still *brittle.*

A new age of anti-forgery is coming though (already here, just needs more support/compute to help execute a global realization), *[Nightshade](https://nightshade.cs.uchicago.edu).* An amazing project along with their *[Glaze](https://glaze.cs.uchicago.edu).* A very dedicated team that aims to battle generative tech and protect the rights of artists. To the point that even screenshots of content will hurt an artificial intelligence’s understanding of visual context. A necessary step of ***consent*** for creators to regain trust within this status quo.

There’s another side to this story. *Deepfakes.* Removing identities from models via a new attack vector. Feeding glazed/night-shaded images into known threat actors training photorealistic generative models. Most of them, well 99% of them, will find it difficult to build new methods of generative algorithms. Primarily working with *forks or fine tuned models.* So, the age of GANs and *now* the age of diffusion is over when it comes to un-ethical training. 

And taking history into account, it will be another 3-5 years to beat [SAND lab’](http://sandlab.cs.uchicago.edu)s methods. In the sense, that they will have to adapt to a new method of image generation surely as well as other parties building attacks upon Glaze itself. Which they will most likely learn about and defend against before the threat actors can adapt, since now the systems are in place. And more often than not, these new methods are founded in institutions, researched heavily. Real threats come after. This is why it’s important to build and fund such forces of AI safety as a *proactive* measure rather than *reactive*.

Bills and laws are one method of doing so, but ironically it can hurt the methods of the “good actors” if they are not thought through. Enabling institutions and labs such as [SAND](http://sandlab.cs.uchicago.edu) to thrive, despite the political climate, is vital. Not just for AI, but for the tech sector as a whole.